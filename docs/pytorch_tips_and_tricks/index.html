<!doctype html><html lang="it" xmlns="http://www.w3.org/1999/xhtml" class="scrolled-to-top"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="description" content=""><title>DaScH-Lab</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><link href="https://fonts.googleapis.com/css?family=Nunito+Sans:300,400,600,700,800,900" rel="stylesheet"><link href="/assets/css/main-6be7509ce1.css" rel="stylesheet"><link href="/assets/css/cookieconsent-219e4c9c7e.css" rel="stylesheet"><script>const setDarkMode=(e=!1)=>{var t=document.querySelector(":root");e?(t.setAttribute("data-theme","dark"),localStorage.setItem("theme","dark")):(t.setAttribute("data-theme","light"),localStorage.setItem("theme","light"))};var queryScheme=window.matchMedia("(prefers-color-scheme: dark)");const themePreference=localStorage.getItem("theme");var activeTheme=queryScheme.matches;"dark"===themePreference&&(activeTheme=!0),"light"===themePreference&&(activeTheme=!1),setDarkMode(activeTheme)</script></head><body class="has-navbar-fixed-top default"><div id="wrapper"><header class="navbar is-fixed-top"><div class="navbar-brand"><a class="navbar-item" href="/">DaScH-Lab</a><div class="navbar-burger" data-target="main-navbar"><span></span> <span></span> <span></span></div></div><div id="main-navbar" class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a> <a class="navbar-item" href="/docs/">Docs</a></div><div class="navbar-end"><div class="navbar-item"><button class="js__dark-mode-toggle dark-mode-toggle" type="button"><span class="dark-mode-toggle__icon"></span> <span class="dark-mode-toggle__text hidden--visually">dark mode</span></button></div><div class="navbar-item"><a href="https://www.toscanalifesciences.org/" target="_blank"><img src="/assets/images/logotls.png" alt="Bulma: a modern CSS framework based on Flexbox" width="112" height="28"></a></div></div></div></header><section id="content"><div class="container"><header id="docsToggles" class="docs-toggle"><button class="button is-primary is-light bd-fat-button is-small side-button" data-target="offside-menu">Show menu</button> <button class="button is-primary is-light bd-fat-button is-small side-button" data-target="offside-toc">Show sidebar</button></header><div class="docs-wrapper"><aside class="docs-side" id="offside-menu"><div class="menu docs-content"><p class="menu-label"><a href="/docs/">Overview</a></p><ul class="menu-list"><li><a href="/docs/conda/">Conda</a></li><li><a href="/docs/nvidia_mig/">Nvidia Multi-Instance GPU</a></li><li><a href="/docs/pymol/">Pymol</a></li><li><a href="/docs/screen/">Screen - Terminal multiplexer</a></li><li><a href="/docs/pytorch_tips_and_tricks/">PyTorch tips and tricks</a></li></ul><p class="menu-label"><a href="/docs/bioinformatics/">Bioinformatics</a></p></div></aside><div class="docs-toc" id="offside-toc"><div class="docs-content"><strong class="mt-2">On this page</strong><hr><div class="toc"><ul><li><a href="#1)-memory-usage">1) Memory usage</a></li><li><a href="#2)-optimizer-step">2) Optimizer step</a></li><li><a href="#3)-loading-and-save-model">3) Loading and save model</a><ul><li><a href="#a.-loading-to-different-device-than-the-model-was-saved-on.">a. Loading to different device than the model was saved on.</a></li><li><a href="#b.-free-the-checkpoint-variable">b. Free the checkpoint variable</a></li></ul></li><li><a href="#4)-asynchronous-data-transfer">4) Asynchronous data transfer</a></li><li><a href="#5)-mixed-precision">5) Mixed precision</a></li><li><a href="#6)-reset-the-gradients-to-none">6) Reset the gradients to None</a></li><li><a href="#7)-gradient-accumulation">7) Gradient accumulation</a></li><li><a href="#8)-from-array-to-torch-tensor">8) From array to torch tensor</a></li><li><a href="#9)-set-the-sizes-of-all-different-architectural-designs-and-batch-sizes-as-the-multiples-of-8">9) Set the sizes of all different architectural designs and batch sizes as the multiples of 8</a></li><li><a href="#10)-setting-torch.backends.cudnn.benchmark-=-true">10) Setting torch.backends.cudnn.benchmark = True</a><ul><li><a href="#further-reading">Further reading</a></li></ul></li></ul></div></div></div><main class="docs-main"><div class="bd-intro"><nav class="navbar" role="navigation" aria-label="page navigation"><div class="navbar-end"><div class="navbar-item"><div class="buttons"><a class="button" href="https://github.com/dasch-lab/dashlab.github.io/tree/master/./src/docs/pytorch_tips_and_tricks.md" title="View and edit this file on GitHub" target="_blank" rel="noopener">View on GitHub</a></div></div></div></nav><h1 class="bd-title mb-0" id="content"></h1></div><div class="container"><h1 id="pytorch-tips-and-tricks" tabindex="-1">PyTorch tips and tricks</h1><p>We collected several PyTorch tips and tricks to handle the many steps required for training/inference processes. Below we provide a list of useful commands<br>with code examples.</p><h2 id="1)-memory-usage" tabindex="-1">1) Memory usage</h2><pre><code>  torch.cuda.memory_allocated(device)
</code></pre><p>We can add this command for each iteration, after forward pass, and so on.</p><pre><code>  print(&quot;Begin: {}&quot;.format(torch.cuda.memory_allocated(device)))
  model.to(device)
  print(&quot;1 - After model to device: {}&quot;.format(torch.cuda.memory_allocated(device)))
</code></pre><p>We can print the difference using variables:</p><pre><code>  a = torch.cuda.memory_allocated(device)
  ...
  do_something
  ...
  b = torch.cuda.memory_allocated(device)
  print(&quot;Memory allocated after do_something: {}&quot;.format(b-a))
</code></pre><h2 id="2)-optimizer-step" tabindex="-1">2) Optimizer step</h2><p>Many optimizers keep track of parameters such as an estimate of the first and second moments of the gradient, for each model weight. For the<br>three main optimizers we have:</p><ul><li>Adam: twice the model size (which uses two moments)</li><li>RMSprop: one times the model size (which uses one moment)</li><li>SGD: zero times the model size (which doesn't uses moments)</li></ul><h2 id="3)-loading-and-save-model" tabindex="-1">3) Loading and save model</h2><h3 id="a.-loading-to-different-device-than-the-model-was-saved-on." tabindex="-1">a. Loading to different device than the model was saved on.</h3><pre><code>  checkpoint = torch.load(save_path, map_location = device)
  model.load_state_dict(checkpoint['model'])
  optimizer.load_state_dict(checkpoint['optimizer'])
</code></pre><h3 id="b.-free-the-checkpoint-variable" tabindex="-1">b. Free the checkpoint variable</h3><pre><code>  del checkpoint
</code></pre><h2 id="4)-asynchronous-data-transfer" tabindex="-1">4) Asynchronous data transfer</h2><p>Use <code>tensor.to(non_blocking=True)</code> when it’s applicable to overlap data transfers and kernel execution.<br>Essentially, <code>non_blocking=True</code> allows asynchronous data transfers to reduce the execution time.</p><h2 id="5)-mixed-precision" tabindex="-1">5) Mixed precision</h2><p>Use mixed precision for forward pass (but not for backward pass)</p><h2 id="6)-reset-the-gradients-to-none" tabindex="-1">6) Reset the gradients to None</h2><p>Set gradients to <code>None</code> before the optimizer updates the weights.</p><h2 id="7)-gradient-accumulation" tabindex="-1">7) Gradient accumulation</h2><p>Update weights for every other <code>x</code> batch to mimic the larger batch size.</p><p>Let's look at an example that contains points 4), 5), 6) and 7).</p><pre><code>  model.train()
  # 6) Reset the gradients to None
  optimizer.zero_grad(set_to_none = True)
  
  scaler = GradScaler()
  
  for i, (features, target) in enumerate(data):
    # 4) non blocking and overlapping
    features = features.to('cuda:0', non_blocking = True)
    target = target.to('cuda:0', non_blocking = True)
    
    # 5) forward pass with mixed precision
    with torch.cuda.amp.autocast():
      output = model(features)
      loss = criterion(output, target)
      
    # Backward pass without mixed precision 
    scaler.scale(loss).backward()
    
    # 7) Only update weights every other two iterations
    if (i+1) % 2 == 0 or (i+1) = len(data):
      # scaler.step() first unscales the gradients.
      # If this gradients contain infs or Nans, optimizer.step() is skipped
      scaler.step(optimizer)
      
      # If optimizer.step() was skipped, scaling factor is reduced by the backoff_factor in GradScaler()
      scaler.update()
      
      # Reset the gradient to None
      optimizer.zero_grad(set_to_none = True)
</code></pre><h2 id="8)-from-array-to-torch-tensor" tabindex="-1">8) From array to torch tensor</h2><p>Use <code>torch.from_numpy(numpy_array)</code> and <code>torch.as_tensor(others)</code> instead of <code>torch.tensor</code>.<br>If both the source device and target device are CPU, <code>torch.from_numpy</code> and <code>torch.as_tensor</code> may not create data copies.<br>If the source data is a NumPy array, it’s faster to use <code>torch.from_numpy(numpy_array)</code>. If the source data is a tensor with<br>the same data type and device type, then <code>torch.as_tensor(others)</code> may avoid copying data if applicable. <code>others</code> can be Python list,<br>tuple, or <code>torch.tensor</code>. If the source and target device are different, then we can use the next tip.</p><pre><code>  torch.from_numpy(numpy_array)
  torch.as_tensor(others)
</code></pre><h2 id="9)-set-the-sizes-of-all-different-architectural-designs-and-batch-sizes-as-the-multiples-of-8" tabindex="-1">9) Set the sizes of all different architectural designs and batch sizes as the multiples of 8</h2><p>To maximize the computation efficiency of GPU, it’s the best to ensure different architecture designs (including the input and<br>output size/dimension/channel numbers of neural networks and batch size) are the multiples of 8 or even larger powers of two<br>(e.g., 64, 128 and up to 256). It’s because the Tensor Cores of Nvidia GPUs achieve the best performance for matrix multiplication<br>when the matrix dimensions align to the multiples of powers of two.</p><h2 id="10)-setting-torch.backends.cudnn.benchmark-=-true" tabindex="-1">10) Setting <code>torch.backends.cudnn.benchmark = True</code></h2><p>Write <code>torch.backends.cudnn.benchmark = True</code> before the training loop can speed up the computation.</p><p>WARNING: it's recommended when the input size doesn't change often, otherwise might hurt the performance.</p><hr><h3 id="further-reading" tabindex="-1">Further reading</h3><ol><li><a href="https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3">https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3</a></li><li><a href="https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6">https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6</a></li></ol></div></main></div><script type="text/javascript">window.addEventListener("load",function(){$(".side-button").click(function(){var t=$(this).data("target");$(this).toggleClass("is-active"),$("#"+t).toggleClass("is-active")})})</script></div></section></div><footer class="footer"><div class="content has-text-centered"><small>The following pages contain links to other features, websites and or content belonging to or originating from third parties: by accessing you agree to the terms and conditions of use set forth therein. <strong>DaScH-Lab</strong> and the TLS disclaims any responsibility and recommend that you carefully read the terms and conditions of use set forth.</small></div></footer><script defer="defer" src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script><script defer="defer" src="/assets/js/cookieconsent-3a7dc6733a.js"></script><script type="text/javascript">window.addEventListener("load",function(){$(".navbar-burger").click(function(){var e=$(this).data("target");$(this).toggleClass("is-active"),$("#"+e).toggleClass("is-active")});const e=()=>{var e=document.querySelector(":root").getAttribute("data-theme");setDarkMode("light"===e)};var t,o;[t=!1]=[activeTheme],o=window.matchMedia("(prefers-color-scheme: dark)"),setDarkMode(t),o.addListener(e=>setDarkMode(e.matches)),document.querySelector(".js__dark-mode-toggle").addEventListener("click",e),initCookieConsent().run({current_lang:"en",autoclear_cookies:!0,page_scripts:!0,cookie_name:"daschlabcookie",languages:{en:{consent_modal:{title:"We use cookies!",description:'Hi, this website uses essential cookies to ensure its proper operation and tracking cookies to understand how you interact with it. The latter will be set only after consent. <button type="button" data-cc="c-settings" class="cc-link">Let me choose</button>',primary_btn:{text:"Accept all",role:"accept_all"},secondary_btn:{text:"Reject all",role:"accept_necessary"}},settings_modal:{title:"Cookie preferences",save_settings_btn:"Save settings",accept_all_btn:"Accept all",reject_all_btn:"Reject all",close_btn_label:"Close",cookie_table_headers:[{col1:"Name"},{col2:"Domain"},{col3:"Expiration"},{col4:"Description"}],blocks:[{title:"Cookie usage 📢",description:'I use cookies to ensure the basic functionalities of the website and to enhance your online experience. You can choose for each category to opt-in/out whenever you want. For more details relative to cookies and other sensitive data, please read the full <a href="/en/privacy/" class="cc-link">privacy policy</a>.'},{title:"Strictly necessary cookies",description:"These cookies are essential for the proper functioning of my website. Without these cookies, the website would not work properly",toggle:{value:"necessary",enabled:!0,readonly:!0}},{title:"Performance and Analytics cookies",description:"These cookies allow the website to remember the choices you have made in the past",toggle:{value:"analytics",enabled:!1,readonly:!1},cookie_table:[{col1:"^_ga",col2:"google.com",col3:"2 years",col4:"description ...",is_regex:!0},{col1:"_gid",col2:"google.com",col3:"1 day",col4:"description ..."}]},{title:"Advertisement and Targeting cookies",description:"These cookies collect information about how you use the website, which pages you visited and which links you clicked on. All of the data is anonymized and cannot be used to identify you",toggle:{value:"targeting",enabled:!1,readonly:!1}},{title:"More information",description:'For any queries in relation to our policy on cookies and your choices, please <a class="cc-link" href="/contacts/">contact us</a>.'}]}}}})})</script></body></html>